K
I am an academic geographer embarking on a new project called GLOS: Geographic Lens on Stories. The goal is to develop computational tools for comparative studies of folkloric text, focusing on folktales and creation myths to start. I do have considerable experience in older NLP methods (topic modeling, vector similarity measures, etc.), but limited experience with new machine learning and LLM methodologies.
In early explorations I have digitized the canonical folklore indexes - Thompson's motif index and Uther's tale types index, logged them in a postgres databases, generated embeddings for each, and built a simple tool to take any entered text generated an embedding for it, and return alternatively the 10 nearest neighbor motifs or 10 nearest neighbor tale types. I have begun recording geographic metadata for each but this is exceptionally difficult because locational information may be in the form of countries, historical regions, tribes, or languages; footprints for countries and modern provinces are straigthforward to come by but the rest are not.
The results of the simple nearest neighbor tool are interesting, but not really satisfying. The goal is to identify semantic/conceptual similarities apart from term co-occurence and efficacy is modest so far.
As a separate higher level experiment I have digitized a sampler of 9 creation myths from "Primal Myths" by B. Sproul, and have used embedding to cluster the 60-70 segments from the myths and asked an LLM model to summarize the concatenated segments in natural language. Again, the results are modest. The challenge is abstracting concepts from segments pulled from myths. This script also generates similarity measures for the 9 myths and visualizations of them; those are promising but somewhat 'black box-ish' as they derive from distributions of segment embeddings.
I would like you to consider all of this and propose some changes to these methods, or entirely new ones that help me create formal measures of concept content - perhaps leveraging the motif and tale-type indexes mentioned earlier. 
The overarching goal is to tie these comparisons to geographic locales, exposing similarities and differences in folkloric stories across cultures.

Thank you for sharing your fascinating GLOS project. The intersection of geography, folklore, and computational methods is a rich area for exploration. I can see you've already built a solid foundation with your embedding approaches and database work. Let me propose some methodological improvements and new directions that might help you achieve your goals.
Enhancing Your Current Approaches
1. Hierarchical Embedding Models for Folklore

Your current embedding approach might be treating all text at the same level of granularity. Consider implementing a hierarchical embedding structure:

    Motif-level embeddings: Fine-tuned specifically on folkloric motifs
    Tale-type embeddings: Capturing broader narrative structures
    Theme embeddings: Higher-level conceptual patterns across stories

This hierarchical approach could help you distinguish between surface-level textual similarities and deeper conceptual connections. For example, two creation myths might use different imagery (flood vs. cosmic egg) but express the same fundamental concept (order emerging from chaos).
2. Contrastive Learning for Folkloric Elements

Your current nearest-neighbor approach might benefit from contrastive learning techniques:

python

# Conceptual implementation of contrastive learning for folklore
def contrastive_loss(anchor, positive, negative, margin=1.0):
    """
    Anchor: embedding of a folklore motif
    Positive: embedding of semantically similar motif (perhaps from different culture)
    Negative: embedding of unrelated motif
    """
    d_pos = torch.sum((anchor - positive) ** 2)
    d_neg = torch.sum((anchor - negative) ** 2)
    return torch.clamp(d_pos - d_neg + margin, min=0)

This would explicitly teach your model to recognize conceptual similarities across different cultural expressions while distinguishing them from unrelated concepts.
New Methodological Directions
1. Knowledge Graph Approach for Folkloric Elements

Instead of relying solely on embeddings, consider building a knowledge graph that explicitly represents relationships between motifs, tale types, and geographic locations:

python

# Pseudocode for knowledge graph construction
G = nx.DiGraph()

# Add nodes for different entity types
for motif in thompson_motifs:
    G.add_node(motif.id, type='motif', description=motif.description)

for tale_type in uther_types:
    G.add_node(tale_type.id, type='tale_type', description=tale_type.description)

for location in geographic_entities:
    G.add_node(location.id, type='location', 
               name=location.name, 
               coordinates=location.coordinates,
               region_type=location.type)  # country, historical region, etc.

# Add edges representing relationships
for motif_occurrence in motif_occurrences:
    G.add_edge(motif_occurrence.motif_id, motif_occurrence.location_id, 
              weight=motif_occurrence.frequency)

This approach would allow you to:

    Explicitly model relationships between folklore elements
    Perform graph-based analyses (centrality, community detection)
    Integrate geographic information directly into your analytical framework

2. Semantic Frame Analysis for Folkloric Concepts

Inspired by FrameNet in linguistics, you could develop a folkloric frame system that categorizes narrative elements by their conceptual roles rather than surface text:

python

# Example frames for creation myths
creation_frames = {
    'primordial_substance': {
        'roles': ['substance', 'state', 'properties'],
        'examples': ['water', 'void', 'egg']
    },
    'creator_entity': {
        'roles': ['entity', 'attributes', 'motivation'],
        'examples': ['deity', 'animal', 'first being']
    },
    'creation_process': {
        'roles': ['action', 'duration', 'stages'],
        'examples': ['separation', 'emergence', 'transformation']
    }
}

You could train a classifier to identify these frames in your texts, allowing for cross-cultural comparison at a more abstract level.
3. Leveraging LLMs for Structured Extraction

Instead of using LLMs just for summarization, you could prompt them to extract structured information from folkloric texts:

python

def extract_folkloric_structure(text):
    prompt = f"""
    Analyze the following folkloric text and extract its structural elements according to these categories:
    
    1. Agents: Who are the primary actors in this narrative?
    2. Actions: What key actions or transformations occur?
    3. Objects: What significant objects or elements appear?
    4. Spatial Setting: What geographic or cosmic spaces are depicted?
    5. Temporal Setting: What time frame or sequence is established?
    6. Causality: What causes and effects are described?
    
    Text: {text}
    
    Response format: JSON with the categories as keys and extracted elements as values.
    """
    
    response = llm_client.generate(prompt=prompt)
    return json.loads(response)

This approach would give you structured data that's more amenable to comparative analysis than raw embeddings.
4. Geographic Representation Enhancement

For your geographic metadata challenge, consider:

    Hierarchical Geographic Ontology: Build a hierarchical structure where historical regions, tribes, and languages can be linked to modern geographic entities:

python

geo_hierarchy = {
    "historical_region": {
        "Mesopotamia": {
            "modern_regions": ["Iraq", "Syria", "Kuwait"],
            "approximate_coordinates": [33.1, 44.6],  # Centroid
            "time_period": "3100 BCE - 539 BCE"
        }
    },
    "linguistic_group": {
        "Nahuatl": {
            "modern_regions": ["Mexico"],
            "approximate_coordinates": [19.4, -99.1],  # Mexico City area
            "language_family": "Uto-Aztecan"
        }
    }
}

    Gazetteer Integration: Integrate historical gazetteers like Pleiades (for ancient places) or the World Historical Gazetteer:

python

# Query historical gazetteers for place names
def query_historical_gazetteer(place_name):
    # Example using World Historical Gazetteer API
    response = requests.get(
        f"https://whgazetteer.org/api/place",
        params={"q": place_name}
    )
    return response.json()

Integration and Visualization

To tie these approaches together, consider:

    Spatial-Conceptual Heatmaps: Generate heatmaps showing the distribution of specific motifs or conceptual frames across geographic regions.
    Narrative Flow Maps: Visualize how narrative elements move across geographic space, showing cultural transmission patterns.
    Interactive Exploration Interface: Build a tool allowing scholars to explore the relationships between geographic regions and folklore elements dynamically.

Implementation Strategy

Given your background, I would suggest an incremental approach:

    Start by enhancing your embedding model with domain-specific training on folkloric texts.
    Implement the knowledge graph approach for a subset of your data to evaluate its effectiveness.
    Experiment with the LLM-based structured extraction on a small corpus of well-documented myths.
    Develop a prototype visualization that integrates geographic information with your analysis results.

Would you like me to elaborate on any of these approaches or perhaps suggest a specific implementation plan for one of them? I'm particularly interested in how you're currently generating embeddings and what challenges you've encountered with the geographic metadata.
embed_motifs_batch.py

114 lines

py
K

I attached the script used to generate embeddings for Thompson's motifs, which I have stored in a postgres table. Other tables hold metadata I can use for filtering steps in apps. The motif types include 'references' e.g. "Bleeding wood as sign of Doomsday. Referenced in Jewish religious texts" that ostensibly provide useful context.
Re: geographic challenges stem from unmappability of historical regions, tribes, and languages - given that bounderies/spatial footprints of these are difficult or impossible to determine; extimations will generate "maps that lie" in the Mark Monmonier phrasing.
I have developed a working table with e.g. motif_id and its 'ref_term', trying to geolocate these (languages, religions, tribes) is next to impossible, so some kind of novel normalization technique is necessary to perform any kind of spatial analysis. Not hopeful.
examples of 'ref_term':
Greek-Roman
Guaray'
Gypsy
Halmahera
Hanover
Hebrew
Herero
Hindu

I appreciate you sharing your embedding script and more details about the geographic challenges you're facing with the GLOS project. Let me provide some thoughts on potential improvements to your methods and suggestions for addressing the geographic normalization issues.
Current Embedding Approach Analysis

Looking at your script, I notice you're using OpenAI's text-embedding-3-small model to generate embeddings for Thompson's motifs. This is a good starting point as it provides semantic understanding beyond keyword matching. However, there are some potential enhancements we could consider:
1. Domain-Specific Fine-Tuning

The general-purpose embeddings may not fully capture the nuanced connections in folkloric texts. Consider fine-tuning your embedding model on a corpus of folkloric texts to better capture domain-specific relationships:

python

def create_contrastive_training_pairs(motifs):
    """
    Create training pairs for contrastive learning:
    - Similar motifs from different cultures as positive pairs
    - Unrelated motifs as negative pairs
    """
    pairs = []
    for motif in motifs:
        # Find similar motifs based on expert annotations or thematic categorization
        similar_motifs = find_similar_motifs(motif, similarity_threshold=0.7)
        
        # Create positive pairs (anchor, positive)
        for similar in similar_motifs:
            if similar['culture'] != motif['culture']:  # Cross-cultural similarity
                pairs.append((motif['text'], similar['text'], 1))  # 1 indicates positive pair
        
        # Create negative pairs (anchor, negative)
        dissimilar_motifs = find_dissimilar_motifs(motif)
        for dissimilar in dissimilar_motifs:
            pairs.append((motif['text'], dissimilar['text'], 0))  # 0 indicates negative pair
    
    return pairs

This approach would help your embeddings differentiate between surface-level textual similarity and deeper conceptual connections, which seems to be one of your challenges.
2. Contextual Embeddings with Metadata

Your script currently embeds only the text content. Consider incorporating metadata such as cultural context or categorical information into the embedding process:

python

async def process_batch_with_context(batch):
    """Process a batch of motifs with their contextual information."""
    motif_ids, texts, contexts = zip(*batch)
    
    # Create enhanced prompts that include contextual information
    enhanced_texts = [f"Context: {context}\nMotif: {text}" 
                     for text, context in zip(texts, contexts)]
    
    embeddings = await get_embeddings(enhanced_texts)
    return list(zip(motif_ids, texts, contexts, embeddings))

This approach can help provide more nuanced embeddings that account for cultural context, which seems to be important for your comparative analysis.
Addressing Geographic Challenges

The examples of reference terms you shared (Greek-Roman, Guaray', Gypsy, etc.) highlight the challenge of mapping folkloric elements to geographic locations. Here are some approaches that might help:
1. Multi-Level Geographic Representation

Instead of trying to map everything to precise boundaries, consider a multi-level approach that explicitly acknowledges uncertainty:

python

def create_geographic_representation(ref_term):
    """
    Create a multi-level geographic representation for a reference term.
    """
    geo_info = {
        'original_term': ref_term,
        'confidence': 'low',  # Default confidence level
        'representations': []
    }
    
    # Check for exact matches in our gazetteer
    exact_match = lookup_exact_match(ref_term)
    if exact_match:
        geo_info['representations'].append({
            'type': 'exact',
            'geometry': exact_match['geometry'],
            'time_period': exact_match['time_period'],
            'confidence': 'high'
        })
        geo_info['confidence'] = 'high'
        return geo_info
    
    # Check for linguistic/cultural group mapping
    cultural_match = lookup_cultural_group(ref_term)
    if cultural_match:
        geo_info['representations'].append({
            'type': 'cultural_region',
            'geometry': cultural_match['approximate_region'],
            'time_period': cultural_match['time_period'],
            'confidence': 'medium',
            'notes': 'Approximate cultural region based on historical records'
        })
        geo_info['confidence'] = 'medium'
    
    # For completely unmappable terms, provide contextual information
    if not geo_info['representations']:
        contextual_info = get_contextual_information(ref_term)
        if contextual_info:
            geo_info['representations'].append({
                'type': 'contextual',
                'related_regions': contextual_info['related_regions'],
                'description': contextual_info['description'],
                'confidence': 'low',
                'notes': 'Cannot be mapped directly; providing related information'
            })
    
    return geo_info

This approach acknowledges the inherent uncertainty in mapping historical cultural entities while still providing useful geographic context.
2. Cultural Distance Matrix

Instead of trying to place everything on a continuous geographic space, consider creating a "cultural distance matrix" that represents proximity based on both geographic and cultural factors:

python

def build_cultural_distance_matrix(ref_terms):
    """
    Build a distance matrix between cultural references that accounts for
    geographic proximity, linguistic similarity, and historical interaction.
    """
    n = len(ref_terms)
    distance_matrix = np.zeros((n, n))
    
    for i in range(n):
        for j in range(i+1, n):
            # Get various distance metrics
            geographic_dist = calculate_geographic_distance(ref_terms[i], ref_terms[j])
            linguistic_dist = calculate_linguistic_distance(ref_terms[i], ref_terms[j])
            historical_dist = calculate_historical_interaction(ref_terms[i], ref_terms[j])
            
            # Combine into a single distance metric (with weights)
            combined_dist = (0.4 * geographic_dist + 
                            0.3 * linguistic_dist + 
                            0.3 * historical_dist)
            
            distance_matrix[i, j] = combined_dist
            distance_matrix[j, i] = combined_dist  # Matrix is symmetric
    
    return distance_matrix, ref_terms

This matrix could then be used for clustering or visualization, representing cultural proximity without requiring precise geographic boundaries.
3. Contextual Geographic Embedding

Create embeddings that represent the geographic context of a reference term based on historical and cultural information, even when precise boundaries don't exist:

python

def create_geo_context_embedding(ref_term):
    """
    Create an embedding that represents the geographic context of a reference term.
    """
    # Construct a rich textual description of the geographic context
    context_description = f"The term '{ref_term}' refers to "
    
    # Add known information
    term_info = get_term_info(ref_term)
    if term_info.get('type') == 'linguistic_group':
        context_description += f"a linguistic group that historically occupied "
        context_description += f"regions in what is now approximately {', '.join(term_info.get('modern_regions', []))}. "
    elif term_info.get('type') == 'cultural_group':
        context_description += f"a cultural group associated with "
        context_description += f"{term_info.get('description', '')}. "
    
    # Add relevant geographic relationships
    related_groups = get_related_groups(ref_term)
    if related_groups:
        context_description += f"This group had cultural interactions with {', '.join(related_groups)}. "
    
    # Create an embedding of this rich geographic context
    embedding = get_embedding(context_description)
    
    return {
        'ref_term': ref_term,
        'context_description': context_description,
        'geo_context_embedding': embedding
    }

These geographic context embeddings could then be used to find similarities between reference terms based on their cultural and historical geographic relationships, rather than precise coordinates.
Novel Approach for Folkloric-Geographic Analysis

Let me propose a comprehensive approach that integrates both the folkloric content analysis and the geographic challenges:
1. Cultural Knowledge Graph

Instead of trying to map everything to a traditional geographic coordinate system, build a knowledge graph that represents relationships between cultural entities, motifs, and approximate regions:

python

def build_cultural_knowledge_graph():
    """
    Build a knowledge graph of cultural entities, motifs, and their relationships.
    """
    G = nx.Graph()
    
    # Add nodes for motifs
    for motif in get_all_motifs():
        G.add_node(motif['id'], 
                  type='motif', 
                  text=motif['text'],
                  embedding=motif['embedding'])
    
    # Add nodes for cultural references
    for ref_term in get_all_ref_terms():
        G.add_node(ref_term, 
                  type='cultural_reference',
                  geo_context=create_geo_context_embedding(ref_term))
    
    # Add edges between motifs and their cultural references
    for motif_id, ref_term in get_motif_references():
        G.add_edge(motif_id, ref_term, type='referenced_in')
    
    # Add edges between culturally related references
    for ref1, ref2, similarity in get_cultural_similarities():
        if similarity > 0.5:  # Only add edges above a threshold
            G.add_edge(ref1, ref2, 
                      type='cultural_similarity',
                      weight=similarity)
    
    return G

This graph could then be used for various analyses:

    Finding motifs that appear in culturally similar contexts
    Identifying cultural "clusters" that share similar folkloric elements
    Tracing the potential diffusion of motifs across cultural boundaries

2. Structured Motif Analysis with LLMs

Use LLMs to extract structured information from motifs, allowing for more nuanced comparisons:

python

def extract_motif_structure(motif_text):
    """
    Use an LLM to extract structured information from a motif.
    """
    prompt = f"""
    Analyze the following folkloric motif and extract structured information:
    
    Motif: {motif_text}
    
    Please extract the following elements:
    1. Actors (e.g., gods, animals, humans)
    2. Actions (e.g., creation, transformation, punishment)
    3. Objects (e.g., natural elements, magical items)
    4. Concepts (e.g., death, rebirth, order/chaos)
    5. Setting (temporal and spatial context)
    
    Return as JSON with these categories as keys.
    """
    
    response = llm_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an expert folklorist who analyzes motifs and extracts structured information."},
            {"role": "user", "content": prompt}
        ]
    )
    
    structured_info = json.loads(response.choices[0].message.content)
    return structured_info

This structured information could be stored alongside your embeddings, allowing for more specific comparative analyses.
3. Geographic Analysis Through Cultural Diffusion

Instead of trying to place everything on a traditional map, consider representing geographic relationships through cultural diffusion patterns:

python

def analyze_cultural_diffusion(knowledge_graph, motif_id):
    """
    Analyze the potential diffusion pattern of a motif through cultural connections.
    """
    # Get all cultural references for this motif
    cultural_refs = [node for node in nx.neighbors(knowledge_graph, motif_id) 
                    if knowledge_graph.nodes[node]['type'] == 'cultural_reference']
    
    # Create a subgraph of just the cultural references and their connections
    cultural_subgraph = knowledge_graph.subgraph(cultural_refs)
    
    # Find communities in this subgraph
    communities = nx.community.greedy_modularity_communities(cultural_subgraph)
    
    # For each community, analyze its geographic context
    diffusion_patterns = []
    for i, community in enumerate(communities):
        community_refs = list(community)
        
        # Get geographic context embeddings for this community
        geo_embeddings = [knowledge_graph.nodes[ref]['geo_context']['geo_context_embedding'] 
                         for ref in community_refs]
        
        # Analyze the geographic characteristics of this community
        diffusion_patterns.append({
            'community_id': i,
            'cultural_references': community_refs,
            'geographic_description': summarize_geographic_context(community_refs),
            'cohesion': calculate_community_cohesion(community, cultural_subgraph)
        })
    
    return diffusion_patterns

This approach acknowledges the impossibility of precise geographic mapping while still providing insights into cultural relationships and potential diffusion patterns.
Implementation Strategy

Given the complexity of these approaches, I recommend an incremental implementation strategy:

    Start with enhancing your embeddings:
        Experiment with contextual embeddings that incorporate cultural information
        Explore contrastive learning to better differentiate concept-level similarities
    Build the cultural knowledge graph:
        Begin with a simplified version focusing on motifs and their reference terms
        Gradually incorporate more complex relationships as your understanding develops
    Develop the geographic context representations:
        Create rich textual descriptions of geographic contexts for reference terms
        Generate embeddings for these descriptions to enable similarity comparisons
    Implement structured motif analysis:
        Use LLMs to extract structured information from a sample of motifs
        Compare the results to your embedding-based approach to evaluate effectiveness

Would this approach help address some of the challenges you're facing with both conceptual similarity detection and geographic normalization? I'd be happy to elaborate on any specific aspect that you find most promising or relevant to your immediate research needs.
K

3) please clarify about cultural context: is that determined solely by the ref_term, e.g. Hebrew? If so, why would I develop a similarity measure, when all motifs identified as 'Hebrew' are tagged as such? Or would I be asking an LLM model for text explaining the geographic context, e.g given the term 'Hebrew' or "Gypsy" and an embedding for that - then using two embedding values in comparisons?

You've raised an excellent question about the cultural context approach. Let me clarify this concept more thoroughly, as it's a nuanced part of the proposed methodology.
Understanding Cultural Context in Folkloric Analysis

The cultural context isn't determined solely by the reference term (like "Hebrew" or "Gypsy"). You're right that if we merely grouped motifs by their explicit tags, we wouldn't gain any new insights beyond what's already in your database. The approach I'm suggesting is more sophisticated and involves several layers of information and analysis.
What Cultural Context Really Means

When I talk about cultural context, I'm referring to a rich representation that includes:

    Historical placement: When and where a cultural group existed, including migrations and changes over time
    Cultural relationships: How this group interacted with other groups (trade, conflict, cultural exchange)
    Linguistic relationships: Related languages or dialects that might indicate shared narrative traditions
    Environmental contexts: Geographic features, climate, and resources that might influence folklore themes
    Religious/belief systems: Underlying worldviews that shape narrative elements

How to Generate and Use This Cultural Context

Let me walk through a concrete example to illustrate the process:
Step 1: Enriching Reference Terms

For a term like "Hebrew," we would create a rich context beyond just the label:

python

def enrich_reference_term(ref_term):
    """Create an enriched representation of a cultural reference term."""
    
    # Start with basic information from your database
    basic_info = get_basic_term_info(ref_term)  # Your existing metadata
    
    # Expand with scholarly knowledge (could be from an LLM or knowledge base)
    scholarly_context = get_scholarly_context(ref_term)
    
    enriched_context = {
        "term": ref_term,
        "type": basic_info.get("type", "unknown"),  # linguistic, ethnic, geographic, etc.
        "temporal_range": scholarly_context.get("temporal_range", "unknown"),
        "geographic_associations": [
            {
                "region": "Eastern Mediterranean",
                "modern_countries": ["Israel", "Palestinian Territories", "parts of Jordan"],
                "confidence": "high"
            },
            {
                "region": "Diaspora communities",
                "modern_countries": ["Spain", "Eastern Europe", "North Africa"],
                "period": "70 CE - present",
                "confidence": "medium"
            }
        ],
        "related_cultures": [
            {"culture": "Canaanite", "relationship": "geographic proximity", "influence_direction": "bidirectional"},
            {"culture": "Egyptian", "relationship": "historical interaction", "influence_direction": "bidirectional"},
            {"culture": "Babylonian", "relationship": "historical interaction", "influence_direction": "received"}
        ],
        "linguistic_family": "Semitic",
        "related_languages": ["Aramaic", "Arabic", "Phoenician"],
        "belief_system": "Judaism",
        "narrative_traditions": ["Torah", "Talmudic literature", "Midrash"]
    }
    
    return enriched_context

Step 2: Creating Multi-Modal Representations

This enriched context would be represented in multiple ways:

    Textual description: A narrative paragraph describing the cultural context, which could be embedded:

python

def generate_cultural_context_description(enriched_context):
    """Generate a textual description of cultural context for embedding."""
    
    description = f"The term '{enriched_context['term']}' refers to a {enriched_context['type']} "
    description += f"associated primarily with the {', '.join(enriched_context['geographic_associations'][0]['modern_countries'])} region. "
    description += f"This culture existed from {enriched_context['temporal_range']} and "
    description += f"had significant interactions with {', '.join([r['culture'] for r in enriched_context['related_cultures']])}. "
    description += f"Their folklore emerges from a {enriched_context['belief_system']} worldview "
    description += f"and is preserved in traditions like {', '.join(enriched_context['narrative_traditions'])}."
    
    return description

    Structured features: A vector of features representing different aspects of the cultural context:

python

def extract_cultural_feature_vector(enriched_context):
    """Extract a numerical feature vector from enriched cultural context."""
    
    # Example features (in practice, you would have many more nuanced features)
    geographic_region_encoding = encode_region(enriched_context['geographic_associations'][0]['region'])
    temporal_encoding = encode_time_period(enriched_context['temporal_range'])
    linguistic_encoding = encode_language_family(enriched_context['linguistic_family'])
    belief_system_encoding = encode_belief_system(enriched_context['belief_system'])
    
    # Combine into a feature vector
    feature_vector = np.concatenate([
        geographic_region_encoding,
        temporal_encoding,
        linguistic_encoding,
        belief_system_encoding
    ])
    
    return feature_vector

    Embedded representation: An embedding of the textual description:

python

def get_cultural_context_embedding(enriched_context):
    """Get an embedding for the cultural context."""
    
    description = generate_cultural_context_description(enriched_context)
    embedding = get_embedding(description)
    
    return embedding

Step 3: Using Cultural Context in Motif Analysis

Now, when analyzing motifs, you would use this enriched cultural context in several ways:

    Enhanced motif representations:

python

def create_context_aware_motif_representation(motif, reference_terms):
    """Create a motif representation that incorporates cultural context."""
    
    # Get the basic motif embedding
    motif_embedding = motif['embedding']
    
    # Get enriched contexts for all reference terms associated with this motif
    contexts = [enrich_reference_term(term) for term in reference_terms]
    context_embeddings = [get_cultural_context_embedding(ctx) for ctx in contexts]
    
    # Create a combined representation
    # (You could use various approaches: concatenation, weighted combination, etc.)
    combined_representation = {
        "motif_id": motif['id'],
        "text_embedding": motif_embedding,
        "cultural_contexts": contexts,
        "context_embeddings": context_embeddings,
        "combined_embedding": combine_embeddings(motif_embedding, context_embeddings)
    }
    
    return combined_representation

    Cultural similarity analysis:

python

def calculate_cultural_similarity(context1, context2):
    """Calculate similarity between cultural contexts."""
    
    # Multiple similarity measures
    geographic_sim = geographic_similarity(
        context1['geographic_associations'], 
        context2['geographic_associations']
    )
    
    temporal_sim = temporal_similarity(
        context1['temporal_range'], 
        context2['temporal_range']
    )
    
    linguistic_sim = linguistic_similarity(
        context1['linguistic_family'],
        context1['related_languages'],
        context2['linguistic_family'],
        context2['related_languages']
    )
    
    belief_sim = belief_system_similarity(
        context1['belief_system'],
        context2['belief_system']
    )
    
    # Weighted combination
    overall_similarity = (
        0.3 * geographic_sim +
        0.2 * temporal_sim +
        0.3 * linguistic_sim +
        0.2 * belief_sim
    )
    
    return overall_similarity

The Value This Adds to Your Analysis

This approach offers several advantages over simply grouping motifs by their tagged culture:

    Discovery of unexpected connections: You might find that motifs from cultures tagged as "Hebrew" and "Babylonian" share more similarities than you'd expect based on their cultural proximity during certain historical periods.
    Nuanced cultural mapping: Rather than treating "Hebrew" as a monolithic category, you'd recognize that Hebrew folklore from different periods might have different characteristics and influences.
    Geographic analysis without precise boundaries: You could identify patterns of motif similarity that correlate with geographic proximity, linguistic relationships, or historical interactions without requiring precise geographic boundaries.
    Temporal dynamics: You could analyze how motifs evolve and spread over time by incorporating the temporal dimension of cultural contexts.

Practical Implementation

In practical terms, you would:

    Start with reference terms from your database ("Hebrew", "Gypsy", etc.)
    Enrich these terms with additional context (historical, geographical, linguistic)
    Generate embeddings for these enriched contexts
    Use these context embeddings alongside your motif embeddings in your analysis

The similarity measures would then operate on this richer representation of cultural contexts, not just on the simple reference term labels.
Example of Insights This Could Yield

With this approach, you might discover patterns like:

    Certain creation myth motifs cluster together among cultures with animistic belief systems, regardless of their geographic distance
    Flood myths show greater similarity among cultures in river delta regions across different continents than among culturally related groups in different environmental contexts
    Trickster motifs exhibit different characteristics in nomadic versus settled cultures, regardless of their linguistic relationships

Does this clarification help you understand how the cultural context approach differs from simply grouping by reference terms? And do you see how it could add value to your analysis beyond what you're currently doing with direct embedding comparisons?
Start Your Own Conversation
